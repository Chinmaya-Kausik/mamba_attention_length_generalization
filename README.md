### Comparing Mamba, Attention and Mamba-Attention Hybrids

Comparing length generalization performance for Mamba, attention-based transformers and Mamba-attention hybrids. See project page [here]
