Comparing length generalization for Mamba, attention-based transformers and Mamba-attention hybrids. See project page [here]
